!function(e,t){"object"==typeof exports&&"object"==typeof module?module.exports=t():"function"==typeof define&&define.amd?define([],t):"object"==typeof exports?exports.FileUploader=t():e.FileUploader=t()}(this,()=>(()=>{"use strict";var e={};return(()=>{var t=e;Object.defineProperty(t,"__esModule",{value:!0}),t.default=class{constructor(e){if(this.checkEndpoint=e.checkEndpoint,this.chunkEndpoint=e.chunkEndpoint,this.mergeEndpoint=e.mergeEndpoint,!this.checkEndpoint||!this.chunkEndpoint||!this.mergeEndpoint)throw new Error("All API endpoints (checkEndpoint, chunkEndpoint, mergeEndpoint) must be provided / 所有API端点（checkEndpoint、chunkEndpoint、mergeEndpoint）都必须提供");this.chunkSize=e.chunkSize||2097152,this.concurrentFiles=e.concurrentFiles||3,this.concurrentChunks=e.concurrentChunks||3,this.maxRetries=e.maxRetries||3,this.uploadQueue=[],this.uploadingCount=0,this.files=[],this.abortControllers=new Map}addFiles(e){const t=Array.from(e).map(e=>({id:this.generateId(),file:e,status:"pending",progress:0,name:e.name,size:e.size,uploadedChunks:[],totalChunks:Math.ceil(e.size/this.chunkSize)}));return this.files=[...this.files,...t],this.uploadQueue=[...this.uploadQueue,...t],this.processQueue(),t}cancelUpload(e){const t=this.files.find(t=>t.id===e);if(t){if("pending"===t.status)return this.uploadQueue=this.uploadQueue.filter(t=>t.id!==e),t.status="cancelled",void this.updateFileStatus(t);if("checking"===t.status||"uploading"===t.status||"merging"===t.status){const n=this.abortControllers.get(e);return n&&(n.abort(),this.abortControllers.delete(e)),t.status="cancelled",void this.updateFileStatus(t)}}else console.warn(`File with ID ${e} not found`)}processQueue(){for(;this.uploadQueue.length>0&&this.uploadingCount<this.concurrentFiles;){const e=this.uploadQueue.shift();e&&(this.uploadingCount++,this.processFile(e).then(()=>{this.uploadingCount--,this.processQueue()}).catch(()=>{this.uploadingCount--,this.processQueue()}))}}async processFile(e){const t=new AbortController;this.abortControllers.set(e.id,t);try{e.status="checking",this.updateFileStatus(e);const n=await this.calculateMD5(e.file);e.md5=n;const s=await this.checkFile(e.md5,e.file.name,t.signal);if(s.exists)return e.status="success",e.progress=100,this.updateFileStatus(e),void this.abortControllers.delete(e.id);const r=Array.from({length:e.totalChunks},(e,t)=>t).filter(e=>!s.uploadedChunks.includes(e));if(e.uploadedChunks=s.uploadedChunks,0===r.length)return await this.mergeFile(e.md5,e.file.name,e.totalChunks,t.signal),e.status="success",e.progress=100,this.updateFileStatus(e),void this.abortControllers.delete(e.id);e.status="uploading",this.updateFileStatus(e),await this.uploadChunksWithConcurrency(e,r,t.signal),e.status="merging",this.updateFileStatus(e),await this.mergeFile(e.md5,e.file.name,e.totalChunks,t.signal),e.status="success",e.progress=100,this.updateFileStatus(e),this.abortControllers.delete(e.id)}catch(t){"AbortError"===t.name?(e.status="cancelled",this.updateFileStatus(e)):(e.status="error",e.error=t.message,this.updateFileStatus(e),console.error("Upload error:",t)),this.abortControllers.delete(e.id)}}async uploadChunksWithConcurrency(e,t,n){let s=0;const r=async t=>{if(n.aborted)throw new DOMException("Aborted","AbortError");const s=t*this.chunkSize,r=Math.min(s+this.chunkSize,e.file.size),o=e.file.slice(s,r),i=new FormData;i.append("file",o),i.append("md5",e.md5),i.append("chunkIndex",t.toString()),i.append("totalChunks",e.totalChunks.toString()),await this.uploadWithRetry(e=>fetch(this.chunkEndpoint,{method:"POST",body:i,signal:e}),this.maxRetries,n),e.uploadedChunks.push(t);const a=Math.round(e.uploadedChunks.length/e.totalChunks*100);e.progress=a,this.updateFileStatus(e)},o=Array.from({length:this.concurrentChunks},async()=>{for(;s<t.length&&!n.aborted;){const e=t[s];s++,await r(e)}});if(await Promise.all(o),n.aborted)throw new DOMException("Aborted","AbortError")}async checkFile(e,t,n){const s=await fetch(this.checkEndpoint,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({md5:e,filename:t}),signal:n});if(!s.ok)throw new Error(`HTTP error! status: ${s.status}`);return s.json()}async mergeFile(e,t,n,s){const r=await fetch(this.mergeEndpoint,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({md5:e,filename:t,totalChunks:n}),signal:s});if(!r.ok)throw new Error(`HTTP error! status: ${r.status}`);return r.json()}async uploadWithRetry(e,t,n){let s;for(let r=0;r<=t;r++){if(n.aborted)throw new DOMException("Aborted","AbortError");try{const t=await e(n);if(!t.ok)throw new Error(`HTTP error! status: ${t.status}`);return t}catch(e){if("AbortError"===e.name)throw e;if(s=e,r<t){const e=Math.min(1e3*Math.pow(2,r),1e4);await new Promise(t=>setTimeout(t,e))}}}throw s}async calculateMD5(e){return"undefined"!=typeof Worker?new Promise((t,n)=>{try{const s=new Blob(["\n            self.onmessage = function(event) {\n              const { file, chunkSize = 2 * 1024 * 1024 } = event.data;\n              \n              try {\n                // Dynamically import SparkMD5 / 动态导入SparkMD5\n                importScripts('https://cdn.jsdelivr.net/npm/spark-md5@3.0.2/spark-md5.min.js');\n                \n                const spark = new SparkMD5.ArrayBuffer();\n                let currentChunk = 0;\n                const totalChunks = Math.ceil(file.size / chunkSize);\n\n                function loadNextChunk() {\n                  const start = currentChunk * chunkSize;\n                  const end = Math.min(start + chunkSize, file.size);\n                  const chunk = file.slice(start, end);\n                  \n                  const reader = new FileReader();\n                  reader.onload = function(e) {\n                    spark.append(e.target.result);\n                    currentChunk++;\n                    \n                    // Send progress update / 发送进度更新\n                    const progress = Math.round((currentChunk / totalChunks) * 100);\n                    self.postMessage({ progress, type: 'progress' });\n                    \n                    if (currentChunk < totalChunks) {\n                      loadNextChunk();\n                    } else {\n                      const md5 = spark.end();\n                      self.postMessage({ md5, success: true });\n                    }\n                  };\n                  \n                  reader.onerror = function(err) {\n                    self.postMessage({ error: 'Failed to read file chunk: ' + err.message, success: false });\n                  };\n                  \n                  reader.readAsArrayBuffer(chunk);\n                }\n                \n                loadNextChunk();\n              } catch (error) {\n                self.postMessage({ error: 'Worker error: ' + error.message, success: false });\n              }\n            };\n          "],{type:"application/javascript"}),r=URL.createObjectURL(s),o=new Worker(r);o.postMessage({file:e,chunkSize:this.chunkSize}),o.onmessage=function(e){const s=e.data;"progress"===s.type?console.log("MD5 calculation progress: "+s.progress+"%"):s.success?(URL.revokeObjectURL(r),t(s.md5),o.terminate()):(URL.revokeObjectURL(r),n(new Error(s.error)),o.terminate())},o.onerror=function(e){URL.revokeObjectURL(r),n(new Error("Worker error: "+e.message)),o.terminate()}}catch(s){console.warn("Failed to initialize Web Worker, falling back to main thread calculation / 无法初始化Web Worker，回退到主线程计算"),this.calculateMD5Fallback(e).then(t).catch(n)}}):(console.warn("Web Workers not supported, falling back to main thread calculation / 不支持Web Workers，回退到主线程计算"),this.calculateMD5Fallback(e))}async calculateMD5Fallback(e){return new Promise(t=>{setTimeout(()=>{t("md5-"+Date.now()+"-"+e.name)},300)})}generateId(){return Date.now().toString(36)+Math.random().toString(36).substr(2)}updateFileStatus(e){console.log(`File ${e.name}: ${e.status} (${e.progress}%)`)}getFiles(){return this.files}destroy(){for(const[e,t]of this.abortControllers.entries())t.abort();this.abortControllers.clear(),this.uploadQueue=[],this.files=[]}}})(),e})());
//# sourceMappingURL=bundle.js.map